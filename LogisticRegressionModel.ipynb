{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NOTE: Make sure you run `pip install -U scikit-image` in your command line \n",
    "# to have the newest version of scikit-image installed.\n",
    "from skimage.io import imsave \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and preprocess\n",
    "\n",
    "\n",
    "\n",
    "The original data have 10 categories, labelled by digit 0 to 9. \n",
    "We choose the image of sign ***three*** and ***four*** as the two categories to train our binary classifier.\n",
    "The label for sign \"three\" is 0, and the label for \"four\" is 1.\n",
    "\n",
    "Run the next cell (**DO NOT** change the code), and it will prepare you the training and testing data needed for the assignment. This part is not graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image data and preprocess\n",
    "X_raw = np.load(open('X.npy', 'rb'))\n",
    "Y_raw = np.load(open('y.npy', 'rb'))\n",
    "print('X_raw shape: {}'.format(X_raw.shape))\n",
    "print('Y_raw shape: {}'.format(Y_raw.shape))\n",
    "\n",
    "# Flatten X_raw and transpose\n",
    "X_data = X_raw.reshape(X_raw.shape[0], -1).T\n",
    "print('X_data shape: {}'.format(X_data.shape))\n",
    "\n",
    "# Trnasponse Y_raw and convert from one-hot labels to integer labels\n",
    "Y_data = Y_raw.T\n",
    "Y_data = np.argmax(Y_data, axis=0).reshape((1, Y_data.shape[1]))\n",
    "print('Y_data shape: {}'.format(Y_data.shape))\n",
    "\n",
    "# Choose the data of first two categories (images for number \"three\" and \"four\") \n",
    "X_cat0 = X_data[:, np.where(Y_data == 0)[1]]\n",
    "Y_cat0 = Y_data[:, np.where(Y_data == 0)[1]]\n",
    "X_cat1 = X_data[:, np.where(Y_data == 6)[1]]\n",
    "Y_cat1 = Y_data[:, np.where(Y_data == 6)[1]]\n",
    "\n",
    "# Convert the label of Y_cat0 to 0, and Y_cat1 to 1\n",
    "Y_cat0 = np.zeros_like(Y_cat0)\n",
    "Y_cat1 = np.ones_like(Y_cat1)\n",
    "\n",
    "print()\n",
    "print('X_cat0 shape: {}'.format(X_cat0.shape))\n",
    "print('Y_cat0 shape: {}'.format(Y_cat0.shape))\n",
    "print('X_cat1 shape: {}'.format(X_cat1.shape))\n",
    "print('Y_cat1 shape: {}'.format(Y_cat1.shape))\n",
    "\n",
    "# Select the first 70% from each category and combine them together as training data, and the rest as test data\n",
    "ind_cat0 = int(0.7 * X_cat0.shape[1])\n",
    "ind_cat1 = int(0.7 * X_cat1.shape[1])\n",
    "\n",
    "X_train = np.concatenate((X_cat0[:, :ind_cat0], X_cat1[:, :ind_cat1]), axis=1)\n",
    "Y_train = np.concatenate((Y_cat0[:, :ind_cat0], Y_cat1[:, :ind_cat1]), axis=1)\n",
    "X_test = np.concatenate((X_cat0[:, ind_cat0:], X_cat1[:, ind_cat1:]), axis=1)\n",
    "Y_test = np.concatenate((Y_cat0[:, ind_cat0:], Y_cat1[:, ind_cat1:]), axis=1)\n",
    "\n",
    "print()\n",
    "print('X_train shape {}'.format(X_train.shape))\n",
    "print('Y_train shape {}'.format(Y_train.shape))\n",
    "print('X_test shape {}'.format(X_test.shape))\n",
    "print('Y_test shape {}'.format(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "$X_{train}$ is a $4096\\times 286$ numpy array. $Y_{train}$ is a $1\\times 286$. They are the train data.\n",
    "\n",
    "$X_{test}$ is a $4096\\times 125$. $Y_{test}$ is a $1\\times 125$. They are the test data.\n",
    "\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|--|\n",
    "|X_raw shape: |(2062, 64, 64)|\n",
    "|Y_raw shape: |(2062, 10)\n",
    "|X_data shape: |(4096, 2062)\n",
    "|Y_data shape: |(1, 2062)\n",
    "|--|--|\n",
    "X_cat0 shape: |(4096, 204)\n",
    "Y_cat0 shape: |(1, 204)\n",
    "X_cat1 shape: |(4096, 207)\n",
    "Y_cat1 shape: |(1, 207)\n",
    "|--|--|\n",
    "X_train shape |(4096, 286)\n",
    "Y_train shape |(1, 286)\n",
    "X_test shape |(4096, 125)\n",
    "Y_test shape |(1, 125)\n",
    "\n",
    "\n",
    "# Visualize\n",
    "\n",
    "The following cell will plot two examples of category 0 (number \"three\") and 1 (number \"four\"), which gives you a sense of the data. This part is not graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize some data\n",
    "img_cat0 = X_cat0[:, 14].reshape((64,64))\n",
    "img_cat1 = X_cat1[:, 14].reshape((64,64))\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(img_cat0)\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(img_cat1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 1. Implement sigmoid function\n",
    "\n",
    "\n",
    "Implement the sigmoid function \n",
    "$a = \\sigma(z) = \\frac{1}{1+e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid function\n",
    "\n",
    "    Arg:\n",
    "    z -- A number or numpy array\n",
    "\n",
    "    Return:\n",
    "    a -- sigmoid(z)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    a = 1/(1+np.exp(-z))\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Evaluate Task 1\n",
    "print('sigmoid(-10) = {}'.format(sigmoid(-10)))\n",
    "print('sigmoid(10) = {}'.format(sigmoid(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "|&nbsp;|&nbsp; |          \n",
    "|--|:--:|\n",
    "|**sigmoid(-10) =**|4.5397868702434395e-05|\n",
    "|**sigmoid(10) =** |0.9999546021312976|\n",
    "\n",
    "---\n",
    "\n",
    "# Task 2. Initialize parameters\n",
    "\n",
    "\n",
    "Implement the function that returns parameter $w$ and $b$ with zero values. The dimension of $w$ is given by the argument *dim*, i.e., it should be a $dim\\times 1$ vector. Note that in numpy the dimension of $w$ should be `(dim, 1)`, but not `(dim,)`.\n",
    "\n",
    "*Hint:* use np.zeros to initialize $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "def init_zeros(dim):\n",
    "    \"\"\"\n",
    "    Initialize parameters w, b to zeros\n",
    "    \n",
    "    Arg:\n",
    "    dim -- size of w \n",
    "    \n",
    "    Returns:\n",
    "    w -- a numpy vector of shape (dim, 1)\n",
    "    b -- a number\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Evaluate Task 2\n",
    "dim = 3\n",
    "w, b = init_zeros(dim)\n",
    "print('w = {}'.format(w))\n",
    "print('b = {}'.format(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "&nbsp;|&nbsp;\n",
    "--|--\n",
    "**w =**|[[0.] <br> [0.] <br> [0.]]\n",
    "**b =**|0.0\n",
    "\n",
    "---\n",
    "# Task 3. Implement forward and backward computation\n",
    "\n",
    "The following function conducts the forward and backward computation for logistic regression.\n",
    "\n",
    "Given input data $X$ and label $Y$, parameters $w$ and $b$, it returns the gradients $dw$ and $db$, and the cost as well.\n",
    "\n",
    "In the forward pass:\n",
    "- The activation is computed by: $A=\\sigma(w^{T}X + b)$\n",
    "- Cost is computed by the formula: $-\\frac{1}{m} \\sum_i^{m}[y^{(i)}\\log(a^{(i)}) + (1 - y^{(i)})\\log(1 - a^{(i)})]$\n",
    "\n",
    "In the backward pass:\n",
    "- $dZ = A - Y$\n",
    "- $dw = \\frac{1}{m}XdZ^{T}$\n",
    "- $db = \\frac{1}{m}\\text{np.sum}(dZ)$\n",
    "\n",
    "The gradients are returned in a Python dict object, whose keys are \"dw\" and \"db\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "def forward_backward(X, Y, w, b):\n",
    "    \"\"\"\n",
    "    Implement the forward and backward passes for logistic regression\n",
    "    \n",
    "    Args:\n",
    "    X -- data of size (64 * 64, number of examples)\n",
    "    Y -- true label vector of size (1, number of examples)\n",
    "    w -- weights, a numpy array of size (64 * 64, 1)\n",
    "    b -- bias, a scalar\n",
    "    \n",
    "    Returns:\n",
    "    grads -- containing gradients, dw and db\n",
    "    cost -- cost for the current pass\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Forward\n",
    "    ### START YOUR CODE ###\n",
    "    A = sigmoid(np.dot(w.T,X) + b)  \n",
    "    cost = -(1/m)*(np.sum((Y*np.log(A)) + (1-Y) *np.log(1-A)))\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    # Backward\n",
    "    ### START YOUR CODE ###\n",
    "    dz = A-Y\n",
    "    dw = 1 / m *(np.dot(X,(A - Y).T))\n",
    "    db = 1 / m *(np.sum(A - Y))\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw': dw, 'db': db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Evaluate Task 3\n",
    "X = np.array([[1,2,-3,0],[0.5,6,-5,0]])\n",
    "Y = np.array([[1,0,1,0]])\n",
    "w = np.array([[1],[2]])\n",
    "b = 0\n",
    "grads, cost = forward_backward(X, Y, w, b)\n",
    "\n",
    "print('dw = {}'.format(grads['dw']))\n",
    "print('db = {}'.format(grads['db']))\n",
    "print('cost = {}'.format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "&nbsp;|&nbsp;\n",
    "--|--\n",
    "**dw =**|[[1.22019716] <br> [2.73509556]]\n",
    "**db =**| 0.09519962669353813\n",
    "**cost =**| 6.9550195708335805\n",
    "\n",
    "---\n",
    "# Task 4. Implement gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "In GD function, call `forward_backward()` function for `num_iters` times. Within each iterationl, parameters $w$ and $b$ are updated. The final parameters and a list of cost values are returned. \n",
    "\n",
    "*Hint: parameters are updated by $w = w - \\alpha * dw$* and $b = b - \\alpha * db$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4\n",
    "def GD(X, Y, w, b, num_iters, alpha, verbose=False):\n",
    "    \"\"\"\n",
    "    Implement gradient descent\n",
    "    \n",
    "    Args:\n",
    "    X -- data of size (64 * 64, number of examples)\n",
    "    Y -- true label vector of size (1, number of examples)\n",
    "    w -- weights, a numpy array of size (64 * 64, 1)\n",
    "    b -- bias, a scalar\n",
    "    num_iters -- number of iterations\n",
    "    alpha -- learning rate\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    costs -- list of all the costs computed during the training, this will be used to plot the learning curve.\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Compute gradient and cost by calling forward_backward function\n",
    "        ##### START YOUR CODE #####\n",
    "        grads, cost = forward_backward(X,Y,w,b)\n",
    "        ##### END YOUR CODE #####\n",
    "        \n",
    "        # Obtain dw and db\n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        # Update parameters\n",
    "        ##### START YOUR CODE #####\n",
    "        w = w - (alpha *dw)\n",
    "        b = b-(alpha*db)\n",
    "        ##### END YOUR CODE #####\n",
    "        \n",
    "        # Record and print cost every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if i % 100 == 0 and verbose:\n",
    "            print(\"Cost after iter {}: {}\".format(i, cost))\n",
    "        \n",
    "    params = {'w': w, 'b': b}\n",
    "        \n",
    "    return params, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Evaluate Task 4\n",
    "params, costs = GD(X, Y, w, b, num_iters=1000, alpha=0.01)\n",
    "print('w = {}'.format(params['w']))\n",
    "print('b = {}'.format(params['b']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output:\n",
    "&nbsp;|&nbsp;\n",
    "--|--\n",
    "**w =**|[[ 0.57327302] <br> [-0.8933432 ]]\n",
    "**b =**| 0.05089921193049401\n",
    "\n",
    "---\n",
    "# Task 5. Implement the function to predict\n",
    "\n",
    "\n",
    "\n",
    "Given new data $X$, parameters $w$ and $b$, the function makes predictions on whether each example in $X$ is 0 or 1.\n",
    "\n",
    "*Hint*:\n",
    "- First, compute the activation using $A = \\sigma(w^{T}X + b)$. The resulting $A$ is a $1\\times m$ matrix, $[a^{(1)},a^{(2)},\\dots,a^{(m)}]$, in which $a^{(i)}$ is the probability that $x^{(i)}=1$.\n",
    "- Second, you need to convert probabilities to real predictions by assigning $Y_{pred}^{(i)}$ to 1 if $a^{(1)}>0.5$, else to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5\n",
    "def predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Implement predict function\n",
    "    \n",
    "    Args:\n",
    "    X -- data of size (64 * 64, number of examples)\n",
    "    w -- weights, a numpy array of size (64 * 64, 1)\n",
    "    b -- bias, a scalar\n",
    "    \n",
    "    Returns:\n",
    "    Y_pred -- a numpy array of size (1, number of examples) containing all predictions (0/1) for all the examples in X\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    Y_pred = np.zeros((1, m))\n",
    "    \n",
    "    # Compute the activation A\n",
    "    ##### START YOUR CODE #####\n",
    "    A = sigmoid(np.dot(w.T,X) + b)\n",
    "    ##### END YOUR CODE #####\n",
    "    \n",
    "    # Convert probabilities to binary predictions\n",
    "    for i in range(A.shape[1]):\n",
    "        ##### START TODO #####\n",
    "        if(A[0][i] <= 0.5):\n",
    "            Y_pred[0, i] = 0\n",
    "        else:\n",
    "            Y_pred[0][i] = 1\n",
    "        ##### END TODO #####\n",
    "    \n",
    "    assert(Y_pred.shape == (1, m))\n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Evaluate Task 5\n",
    "print('predictions = {}'.format(predict(X, w, b)))\n",
    "print('predictions = {}'.format(predict(X, params['w'], params['b'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "&nbsp;|&nbsp;\n",
    "--|--\n",
    "**predictions** | [[1. 1. 0. 0.]]\n",
    "**predictions** | [[1. 0. 1. 1.]]\n",
    "\n",
    "\n",
    "---\n",
    "# Task 6. Integrate into one model\n",
    "\n",
    "\n",
    "\n",
    "Integrate the above parts into one model, and apply the model on the train data ($X_{train}, Y_{train}$), and then evaluate on test data ($X_{test},Y_{test}$).\n",
    "\n",
    "*Hint:*\n",
    "- You need to call init_zeros and GD in order. You should pass the initialized parameters and all the other necessary arguments to GD.\n",
    "- You also need to compute the accuray on train and test data respectively. Accuray is defined by the fraction of correct predictions over total number of examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iters=2000, alpha=0.005, verbose=False):\n",
    "    \"\"\"\n",
    "    Integrated model\n",
    "    \n",
    "    Args:\n",
    "    X_train -- training data of size (4096, 286)\n",
    "    Y_train -- training label of size (1, 286)\n",
    "    X_test -- test data of size (4096, 125)\n",
    "    Y_test -- test label of size (1, 125)\n",
    "    \n",
    "    Returns:\n",
    "    result -- a dict object that contains useful information\n",
    "    \"\"\"\n",
    "    ##### START YOUR CODE #####\n",
    "    # Initialize parameters to zeros\n",
    "    w, b = init_zeros(X_train.shape[0])\n",
    "    \n",
    "    # Conduct gradient descent\n",
    "    params, costs = GD(w, b, X_train, Y_train, num_iters=2000, alpha=0.005, verbose=False)\n",
    "    \n",
    "    # Retrieve parameters\n",
    "    w = params['w']\n",
    "    b = params['b']\n",
    "    \n",
    "    # Use the parameters to predict on train and test data\n",
    "    Y_pred_train = predict(w, b, X_test)\n",
    "    Y_pred_test = predict(w, b, X_train)\n",
    "    \n",
    "    # Compute the accuracies of predicting on train/test data\n",
    "    # Accuracy is the fraction of correct predictions over all examples\n",
    "    acc_train = (100-np.mean(np.abs(Y_pred_train-Y_train)))\n",
    "    acc_test = (100-np.mean(np.abs(Y_pred_test-Y_test)))\n",
    "    ##### END YOUR CODE #####\n",
    "    \n",
    "    # Print train/test accuracies\n",
    "    print('train accuracy: {} %'.format(100 * acc_train))\n",
    "    print('test accuracy: {} %'.format(100 * acc_test))\n",
    "    \n",
    "    result = {\n",
    "        'w': w,\n",
    "        'b': b,\n",
    "        'costs': costs,\n",
    "        'Y_pred_test': Y_pred_test\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Evaluate Task 6\n",
    "res = model(X_train, Y_train, X_test, Y_test, num_iters=1500, alpha=0.002, verbose=True)\n",
    "\n",
    "# Plot learning curve\n",
    "costs = np.squeeze(res['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title('Learning rate = 0.002')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "&nbsp;|&nbsp;\n",
    "--|--\n",
    "Cost after iter 0: |0.6931471805599454\n",
    "Cost after iter 100: |0.5946773825987639\n",
    "Cost after iter 200: |0.5256364501984687\n",
    "Cost after iter 300: |0.4747208768166399\n",
    "Cost after iter 400: |0.435436416758632\n",
    "Cost after iter 500: |0.40399872095331557\n",
    "Cost after iter 600: |0.37811027839268685\n",
    "Cost after iter 700: |0.35630887692114865\n",
    "Cost after iter 800: |0.3376209341419335\n",
    "Cost after iter 900: |0.32137148224069756\n",
    "Cost after iter 1000: |0.30707586651947666\n",
    "Cost after iter 1100: |0.29437547177794215\n",
    "Cost after iter 1200: |0.28299807348845724\n",
    "Cost after iter 1300: |0.27273248705908887\n",
    "Cost after iter 1400: |0.26341182071904296\n",
    "train accuracy: | 94.05594405594405 %\n",
    "test accuracy: | 88.0 %\n",
    "\n",
    "<br>\n",
    "<img src=\"lc.png\">\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "# Task 7. Calculate evaluation metrics\n",
    "\n",
    "\n",
    "\n",
    "Calculate 8 evaluation metrics out of the previous results stored in the \"res\" object, using the ground truth label $Y_{test}$ and the predictions on $Y_{test}$, which is stored in res['Y_pred_test'].\n",
    "\n",
    "**NOTE**: We assumte that label y = 1 is positive, and y = 0 is negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7.\n",
    "# Calculate TP, FP, TN, FN, Accuracy, Precision, Recall, and F-1 score\n",
    "# We assume that label y = 1 is positive, and y = 0 is negative\n",
    "def calc_metrics(Y_test, Y_pred_test):\n",
    "    \"\"\"\n",
    "    Calculate metrics\n",
    "    \n",
    "    Args:\n",
    "    Y_test -- test label\n",
    "    Y_pred_test -- predictions on test data\n",
    "    \n",
    "    Return:\n",
    "    metrics -- a dict object\n",
    "    \"\"\"\n",
    "    assert(Y_test.shape == Y_pred_test.shape)\n",
    "    \n",
    "    ##### START YOUR CODE #####\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    Accuracy = 0\n",
    "    Precision = 0\n",
    "    Recall = 0\n",
    "    F1 = 0\n",
    "    ##### END YOUR CODE #####\n",
    "    \n",
    "    metrics = {\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'TN': TN,\n",
    "        'FN': FN,\n",
    "        'Accuracy': Accuracy,\n",
    "        'Precision': Precision,\n",
    "        'Recall': Recall,\n",
    "        'F1': F1\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Evaluate Task 7\n",
    "m = calc_metrics(Y_test, res['Y_pred_test'])\n",
    "print('TP = {}, FP = {}, TN = {}, FN = {}, \\nAccuracy = {}, Precision = {}, Recall = {}, F1 = {}'.format(\n",
    "    m['TP'], m['FP'], m['TN'], m['FN'], m['Accuracy'], m['Precision'], m['Recall'], m['F1']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "&nbsp;|&nbsp;|&nbsp;|&nbsp;\n",
    "--|--|--|--\n",
    "TP = 59 | FP = 11 | TN = 51 | FN = 4\n",
    "Accuracy = 0.88 | Precision = 0.8428571428571429 | Recall = 0.9365079365079365 | F1 = 0.887218045112782"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
